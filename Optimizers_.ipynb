{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAJJ1KMN0wNZ5v1DeipRDx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loki-33/Optimizer/blob/main/Optimizers_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYr_ANQmEYSO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1234)\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "metadata": {
        "id": "XgUK1bsEEgks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = 0.13066048920154572\n",
        "std = 0.30810779333114624"
      ],
      "metadata": {
        "id": "KSRQ1EvSNad8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.RandomCrop(28, padding=2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean], std=[std])\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST(root='.data', train=True, download=True, transform=train_transforms)"
      ],
      "metadata": {
        "id": "DNY3dynqNspa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "train_iterator = data.DataLoader(train_data, shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "13AEoZUqOBQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, hid_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Linear(input_dim, hid_dim)\n",
        "    self.layer2 = nn.Linear(hid_dim, hid_dim)\n",
        "    self.layer3 = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "\n",
        "  def init_params(self):\n",
        "    for n, p in self.named_parameters():\n",
        "      if 'weight' in n:\n",
        "        nn.init.kaiming_normal_(p, nonlinearity='relu')\n",
        "      elif 'bias' in n:\n",
        "        nn.init.constant_(p.data, 0.0)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, *_ = x.shape\n",
        "    x = x.view(batch_size, -1)\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = F.relu(self.layer2(x))\n",
        "    x = self.layer3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "5OoLLEluOHr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 28*28\n",
        "hid_dim = 256\n",
        "output_dim = 10\n",
        "\n",
        "model = MLP(input_dim, hid_dim, output_dim)"
      ],
      "metadata": {
        "id": "dV36pFauOs1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EbRPDwIHO7Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "RV_fmmdjPIcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(iterator, model, optimizer, criterion, device):\n",
        "  losses = []\n",
        "  for images, labels in tqdm.tqdm(iterator):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(images)\n",
        "    loss = criterion(predictions, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "  return losses"
      ],
      "metadata": {
        "id": "BK5AbGlMPPK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_iterator, model, optimizer, criterion, device, n_epochs=3):\n",
        "  losses = []\n",
        "  model.init_params()\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    epoch_losses = train_epoch(train_iterator, model, optimizer, criterion, device)\n",
        "    losses.extend(epoch_losses)\n",
        "  return losses"
      ],
      "metadata": {
        "id": "ssmoLgdSPW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(loss, title=None, ymin=0, ymax=None, figsize=(15,5)):\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  ax.plot(loss)\n",
        "  ax.set_ylabel('Loss')\n",
        "  ax.set_xlabel('Update Steps')\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylim(ymin, ymax)\n",
        "  ax.grid()"
      ],
      "metadata": {
        "id": "gLF61w4PP0Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(losses, labels, title=None, ymin=0, ymax=None, figsize=(15,5)):\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  for loss, label in zip(losses, labels):\n",
        "    ax.plot(loss, label=label)\n",
        "  ax.set_title(title)\n",
        "  ax.set_ylabel('Loss')\n",
        "  ax.set_xlabel('Update Steps')\n",
        "  ax.set_ylim(ymin=ymin, ymax=ymax)\n",
        "  ax.grid()\n",
        "  ax.legend(loc='upper right')\n"
      ],
      "metadata": {
        "id": "xHoFJzUgP0LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "  def __init__(self, model_params, lr=1e-3):\n",
        "    self.model_params = list(model_params)\n",
        "    self.lr = lr\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.model_params:\n",
        "      p.grad = None\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for p in self.model_params:\n",
        "      p.sub_(self.lr * p.grad)"
      ],
      "metadata": {
        "id": "waoyGU71P0R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD(model.parameters())"
      ],
      "metadata": {
        "id": "JpQSc80ZP0W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_loss= train(train_iterator, model, optimizer, criterion, device)"
      ],
      "metadata": {
        "id": "i28DhW0mWEqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(sgd_loss, 'SGD with lr=1e-3')"
      ],
      "metadata": {
        "id": "Cb9IwC57WKXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD_Momentum:\n",
        "  def __init__(self, model_params, lr=1e-3, momentum=0.9):\n",
        "    self.model_params = list(model_params)\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "    self.v = [torch.zeros_like(p) for p in self.model_params]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.model_params:\n",
        "      p.grad = None\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for p,v in zip(self.model_params, self.v):\n",
        "      v.mul_(self.momentum).add_(p.grad)\n",
        "      p.sub_(self.lr*v)"
      ],
      "metadata": {
        "id": "hrlOA1kEYoiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = SGD_Momentum(model.parameters())"
      ],
      "metadata": {
        "id": "mKiuShDkWtN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SGD_momentum_loss = train(train_iterator, model, optimizer, criterion, device)"
      ],
      "metadata": {
        "id": "LO_X2oL7bojg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(SGD_momentum_loss, \"SGD MOMENTUM WITH LR=1e-3 AND MOMENTUM=0.9\")"
      ],
      "metadata": {
        "id": "pPgzjD40bu-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "  def __init__(self, model_params, init_acc_sqr_grad=0, lr=1e-3, eps=1e-10):\n",
        "    self.model_params = list(model_params)\n",
        "    self.lr = lr\n",
        "    self.acc_seq_grads = [torch.full_like(p, init_acc_sqr_grad) for p in self.model_params]\n",
        "    self.eps = eps\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.model_params:\n",
        "      p.grad = None\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for p, a in zip(self.model_params, self.acc_seq_grads):\n",
        "      a.add_(p.grad * p.grad)\n",
        "      std = a.sqrt().add(self.eps)\n",
        "      p.sub_((self.lr / std) * p.grad)"
      ],
      "metadata": {
        "id": "xFXvjKgSeFTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdaGrad(model.parameters())\n"
      ],
      "metadata": {
        "id": "zeSO0xlhfIXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adagrad_loss = train(train_iterator, model, optimizer, criterion, device)\n"
      ],
      "metadata": {
        "id": "9XMkn8xFfKNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(adagrad_loss, 'Adagrad with lr=1e-2, init_acc_sqr_grad=0, eps=1e-10')"
      ],
      "metadata": {
        "id": "6O9oRnxifNLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaDelta:\n",
        "  def __init__(self, model_params, lr=1.0, rho=0.9, eps=1e-9):\n",
        "    self.model_params = list(model_params)\n",
        "    self.lr = lr\n",
        "    self.rho = rho\n",
        "    self.avg_sqr_grads = [torch.zeros_like(p) for p in self.model_params]\n",
        "    self.avg_sqr_deltas = [torch.zeros_like(p) for p in self.model_params]\n",
        "    self.eps = eps\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.model_params:\n",
        "      p.grad = None\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for p, a, b in zip(self.model_params, self.avg_sqr_grads, self.avg_sqr_deltas):\n",
        "      a.mul_(self.rho).add_(p.grad*p.grad * (1 - self.rho))\n",
        "      std = a.add(self.eps).sqrt()\n",
        "      delta = b.add(self.eps).sqrt().div(std).mul(p.grad)\n",
        "      p.sub_(self.lr * delta)\n",
        "      b.mul_(self.rho).add_(delta * delta * (1-self.rho))"
      ],
      "metadata": {
        "id": "LZT-fgRbfOS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdaDelta(model.parameters())"
      ],
      "metadata": {
        "id": "Jlyg85kYqEdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adadelta_loss = train(train_iterator, model, optimizer, criterion, device)"
      ],
      "metadata": {
        "id": "FbJ3Cql4qEhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(adadelta_loss, 'Adadelta with lr=1.0, rho=0.9, eps=1e-6')"
      ],
      "metadata": {
        "id": "QmXaUr07qEt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSprop:\n",
        "  def __init__(self, model_params, lr=1e-2, alpha=0.99, eps=1e-8):\n",
        "    self.model_params = model_params\n",
        "    self.eps = eps\n",
        "    self.lr = lr\n",
        "    self.alpha = alpha #rho in case of AdaDelta\n",
        "    self.avg_sqr_grads = [torch.zeros_like(p) for p in self.model_params]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.model_params:\n",
        "      p.grad = None\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for p, a in zip(self.model_params, self.avg_sqr_grads):\n",
        "      a.mul_(self.alpha).add_(p.grad * p.grad * (1-self.alpha))\n",
        "      std = a.sqrt().add(self.eps)\n",
        "      p.sub_((self.lr / std) * p.grad)"
      ],
      "metadata": {
        "id": "KcUf94yXqEwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = RMSprop(model.parameters())"
      ],
      "metadata": {
        "id": "WIo1NR95qE9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop_loss = train(train_iterator, model, optimizer, criterion, device)"
      ],
      "metadata": {
        "id": "OqcCjoe9wfRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(rmsprop_loss, 'RMSprop with lr=1e-2, alpha=0.99, eps=1e-8')"
      ],
      "metadata": {
        "id": "YnygVnSxwk-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "  def __init__(self, model_params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "    self.model_params = model_params\n",
        "    self.lr = lr\n",
        "    self.beta1 = beta1\n",
        "    self.beta2 = beta2\n",
        "    self.eps = eps\n",
        "    self.n_steps = 0\n",
        "    self.avg_grads = [torch.zeros_like(p) for p in self.model_params]\n",
        "    self.avg_sqr_grads = [torch.zeros_like(p) for p in self.model_params]\n",
        "\n",
        "  def zero_grad(self):\n",
        "    for p in self.model_params:\n",
        "      p.grad = None\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def step(self):\n",
        "    for p, a, b in zip(self.model_params, self.avg_grads, self.avg_sqr_grads):\n",
        "      self.n_steps += 1\n",
        "      a.mul_(self.beta1).add_(p.grad * (1-self.beta1))\n",
        "      b.mul_(self.beta2).add_(p.grad * p.grad * (1-self.beta2))\n",
        "      avg_corrected_grad = a.div(1-self.beta1**self.n_steps)\n",
        "      avg_corrected_sqr_grad = b.div(1-self.beta2**self.n_steps)\n",
        "      std = avg_corrected_sqr_grad.sqrt().add(self.eps)\n",
        "      p.sub_(self.lr * avg_corrected_grad / std)"
      ],
      "metadata": {
        "id": "vGig5giE5f2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters())"
      ],
      "metadata": {
        "id": "rHwOEsAq5f8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adam_loss = train(train_iterator, model, optimizer, criterion, device)"
      ],
      "metadata": {
        "id": "PkIMmQvj5gBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(adam_loss, 'Adam with lr=1e-3, betas=(0.9, 0.999), eps=1e-8')"
      ],
      "metadata": {
        "id": "tjXsOgDj5gF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "30ucyw7P5gKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}